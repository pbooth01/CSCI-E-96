{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c61735ea",
   "metadata": {},
   "source": [
    "# Assignment 06\n",
    "\n",
    "## Recommender Systems \n",
    "\n",
    "## CSCI S-96\n",
    "\n",
    "> Instructions: For this assignment you will complete the exercises shown. All exercises involve creating and executing some Python code. Additionally, most exercises have questions for you to answer. You can answer questions by creating a Markdown cell and writing your answer. If you are not familiar with Markdown, you can find a brief tutorial here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d22f6",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "Recommender algorithms are widely used in commerce. Further, the basic algorithms have found applications in other areas. In this Assignment you gain some experience working with the nature of recommender data, the collaborative filtering algorithm and the versions of the nonnegative matrix factorization algorithm.           \n",
    "\n",
    "This Assignment uses the Python [Surprise](http://surpriselib.com/) package a scikit for recommender system experimentation. You can find further installation instructions on this linked page. If you have not previously installed Surprise uncomment the code in the cell below and execute it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c209b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install the surprise package if it is not already insalled  \n",
    "## Uncommend the line of code below and exectue this cell\n",
    "#!pip3 install scikit-surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1794d61",
   "metadata": {},
   "source": [
    "Execute the code in the cell below to import the packages you will need for this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c703c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.prediction_algorithms.matrix_factorization import SVD, NMF\n",
    "from surprise.prediction_algorithms.baseline_only import BaselineOnly\n",
    "from surprise.prediction_algorithms.knns import KNNBasic\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187e0e31",
   "metadata": {},
   "source": [
    "## Load and Explore the Dataset   \n",
    "\n",
    "In this Assignment you will work with the MovieLens 100k dataset. This dataset includes a total of 100,000 movie ratings from IMDB. \n",
    "\n",
    "Before working with recommender algorithms, will explore these data. Exploration of data is an essential step in any data mining process.     \n",
    "\n",
    "The code in the cell below loads the dataset and splits it into training and testing datasets. Execute this code. If you are asked to if you want to download the data you must answer y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13593f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the movielens-100k dataset \n",
    "## Answer y if you are asked to download the dataset\n",
    "movie_lense_data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "## Split the dataset into training and test subsets.\n",
    "np.random.seed(4517)\n",
    "ml_trainset, ml_testset = train_test_split(movie_lense_data, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d69e4e",
   "metadata": {},
   "source": [
    "Before experimenting with the recommender algorithms we will explore the data. As a first step of the execute the code in the cell below to print some characteristics of the training dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a87679",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of users = {}\".format(ml_trainset.n_users))\n",
    "print(\"Number of items = {}\".format(ml_trainset.n_items))\n",
    "print(\"Number of ratings = {}\".format(ml_trainset.n_ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dd669a",
   "metadata": {},
   "source": [
    "You can see some basic characteristics of the training dataset. If these data were represented as a rectangular table or matrix they would require, over 1.5 million, and at least that many bytes. In practice, a more efficient representation is used. \n",
    "\n",
    "The Surprise package, like all effective recommender software, uses a space efficient representation. The `nr` attribute of the Surprise data object contains a dictionary with the recommender data. The keys of the dictionary are numeric identifiers of the users. The values of this key-value pairs are a list of tuples. The tuples are key value pairs, with the key being the item identifier and the value the rating. In summary, there is a key to value of a key value pair.      \n",
    "\n",
    "The code in the cell below prints the list of key value, item identifier, rating, value pairs for user identifier 5. Execute this code and examine the results.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fa930b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Print the product-rating tupples in the 'ur' attribute of the data object from user 6\n",
    "print(len(ml_trainset.ur[5]))\n",
    "ml_trainset.ur[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79af1c7",
   "metadata": {},
   "source": [
    "User 5 has only rated 32 items of 1647 total items.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f28953",
   "metadata": {},
   "source": [
    "> **Exercise 06-1:** To gain some understanding of the nature of the ratings data do the following:    \n",
    "> 1. In the cell below create and execute code to compute and print the decimal fraction of items rated by user 5 and the decimal fraction of the average number of items rated by all users. Use the attributes of the training dataset to extract the required parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6870b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7ae678",
   "metadata": {},
   "source": [
    "> 2. Given that real-world online catalogs (products, videos, music, etc.) can contain millions of items, do you think these fractions for an individual user and the average user are realistic or too high? \n",
    "> 3. Create two lists with the following values, a) list named `mean_ratings` containing the mean rating of each user, and b) a list named `number_ratings` that contains the count of ratings for each user. The user ratings are represented with a Python dictionary, with key the user identifier in the `ur` attribute of the training dataset. You can extract the keys (user identifiers) with the `keys` method.  \n",
    "> 4. Once you have completed your code, execute the code in the cell below to display the histograms.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4639c3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the mean rating for each of the users in the training dataset\n",
    "mean_ratings = []\n",
    "number_ratings = []\n",
    "## Put your code here\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "## Create a histograms of the results\n",
    "fig,ax= plt.subplots(1,2,figsize=(15,4))\n",
    "_=ax[0].hist(mean_ratings, bins=20, alpha=0.5) \n",
    "_=ax[0].set_xlabel('Mean user rating')\n",
    "_=ax[0].set_ylabel('Frequency')\n",
    "_=ax[0].set_title('Distribution of mean user ratings')\n",
    "_=ax[1].hist(number_ratings, bins=20, alpha=0.5) \n",
    "_=ax[1].set_xlabel('Number of items rated')\n",
    "_=ax[1].set_ylabel('Frequency')\n",
    "_=ax[1].set_title('Distribution of number of items rated by user')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23fd56",
   "metadata": {},
   "source": [
    "> 5. Is the distribution of the mean user ratings a bit skewed? Do you think this skew is reasonable?   \n",
    "> 6. Notice the rapid decay in the frequency of items rated by users. Why is this result considered reasonable?  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cceab1c",
   "metadata": {},
   "source": [
    "## Baseline Only Model\n",
    "\n",
    "One possible answer to the cold start problem is to use a **baseline only** model. In this model we just use the average or baseline values for users or items to predict ratings. This same approach can be applied to users who consume an item, but provide no rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3cc000",
   "metadata": {},
   "source": [
    "> **Exercise 06-2:** You will now create and evaluate a baseline recommender model. Do the following:   \n",
    "> 1. Instantiate a [surprise.prediction_algorithms.baseline_only.BaselineOnly](surprise.prediction_algorithms.baseline_only.BaselineOnly) model object and it with the `fit` method using the `ml_testset` as the argument. Name your model object `baseline_model`.  \n",
    "> 2. Compute the recommender predictions with the `test` method using the `ml_testset` as the argument.   \n",
    "> 3. Compute and print the RMSE using [surprise.accuracy.msee](https://surprise.readthedocs.io/en/stable/accuracy.html) and MAE using [surprise.accuracy.mae](https://surprise.readthedocs.io/en/stable/accuracy.html) with the predictions as the argument in both cases.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92d5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate the model object and fit to the training data  \n",
    "# Put your code below\n",
    "\n",
    "\n",
    "\n",
    "## Compute the predicitons from the model \n",
    "# Put your code below\n",
    "\n",
    "\n",
    "# Compute some model performance statistics\n",
    "# Put your code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9ad54e",
   "metadata": {},
   "source": [
    "> 4. Does the performance of this model seem good given its simplicity?   \n",
    "> 5. Execute the code in the cell below to display histograms of the item and user biases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d87746",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the biases\n",
    "fig,ax= plt.subplots(1,2,figsize=(15,4))\n",
    "_=ax[0].hist(baseline_model.bi, bins=20, alpha=0.5) \n",
    "_=ax[0].set_xlabel('Item bias')\n",
    "_=ax[0].set_ylabel('Frequency')\n",
    "_=ax[0].set_title('Distribution of item bias')\n",
    "_=ax[0].set_xlim(-1.5,1.5)\n",
    "_=ax[1].hist(baseline_model.bu, bins=20, alpha=0.5) \n",
    "_=ax[1].set_xlabel('User bias')\n",
    "_=ax[1].set_ylabel('Frequency')\n",
    "_=ax[1].set_title('Distribution of user bias')\n",
    "_=ax[1].set_xlim(-1.5,1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820d4896",
   "metadata": {},
   "source": [
    "> 6. These bias values are approximately Normally distributed, but with somewhat heavy tails. Examine the range of these biases. What does the fact that these ranges are small compared to the range of ratings tell you about the magnitude of the bias adjustments?    \n",
    "> 7. Ideally, the biases should be uniformly distributed with identifier. In other words, statistically independent of the identifiers. Execute the code to display the relationship.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea4f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the biases\n",
    "fig,ax= plt.subplots(1,2,figsize=(15,4))\n",
    "_=ax[0].scatter(range(len(baseline_model.bi)), baseline_model.bi) \n",
    "_=ax[0].set_xlabel('Item identifier')\n",
    "_=ax[0].set_ylabel('Bias value')\n",
    "_=ax[0].set_title('Item bias vs. item identifier')\n",
    "_=ax[1].scatter(range(len(baseline_model.bu)), baseline_model.bu) \n",
    "_=ax[1].set_xlabel('User identifier')\n",
    "_=ax[1].set_ylabel('Bias value')\n",
    "_=ax[1].set_title('User bias vs. user identifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12326c3",
   "metadata": {},
   "source": [
    "> 8. Examine these plots. Do the distributions look largely uniform with identifier or not?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f394b431",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Model  \n",
    "\n",
    "The collaborative filtering model is an unsupervised learning model using similarity item-item or user-user similarity measures. Items with the highest similarity measures are recommended.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cbee48",
   "metadata": {},
   "source": [
    "> **Exercise 06-3:** You will now create and evaluate a baseline recommender model. Do the following:   \n",
    "> 1. Instantiate a [surprise.prediction_algorithms.knns.KNNBasic](https://surprise.readthedocs.io/en/stable/knn_inspired.html) model object and it with the `fit` method using the `ml_testset` as the argument. Name your model object `collaborative_filter_model`.  \n",
    "> 2. Compute the recommender predictions with the `test` method using the `ml_testset` as the argument.   \n",
    "> 3. Compute and print the RMSE using `surprise.accuracy.msee` and MAE using `surprise.accuracy.mae` with the predictions as the argument in both cases.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472e0513",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate the model object and fit to the training data  \n",
    "## Put your code here\n",
    "\n",
    "\n",
    "\n",
    "## Compute the predicitons from the model \n",
    "## Put your code here\n",
    "\n",
    "\n",
    "# Compute some model performance statistics\n",
    "## Put your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a130312",
   "metadata": {},
   "source": [
    "> 4. Compare the performance of this model to the baseline model. Is it better or worse? Keeping in mind that the rating sampling is more dense that a real-world example, what does this tell you about the effectiveness of a baseline model vs. the model based on similarity.    \n",
    "> 5. The distribution of the similarities gives insight into the behavior of the model. Execute the code in the to display the histogram of the user-user similarity measures.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7db96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plt.hist(collaborative_filter_model.sim.flatten(), bins=40, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34716d92",
   "metadata": {},
   "source": [
    "> 6. Examine the plot. Notice there are a number of similarities which are or are close to 0 or 1. Is the distribution of these similarities skewed? If so, what does this skew tell you about how strong a predictor similarity can be in this case.    \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f7c1c1",
   "metadata": {},
   "source": [
    "## Matrix Factorization Methods   \n",
    "\n",
    "Matrix factorization algorithms are generally considered to be the state of the art for recommenders. The matrix factorization methods find a set of latent (non-observable) factor variable values, which are used to compute rating estimates. Additionally, user and item rating bias adjustments are applied.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52747377",
   "metadata": {},
   "source": [
    "> **Exercise 06-4:** You will now create and evaluate a baseline recommender model. Do the following:   \n",
    "> 1. Instantiate a [surprise.prediction_algorithms.matrix_factorization.SVD](https://surprise.readthedocs.io/en/stable/matrix_factorization.html) model object and it with the `fit` method using the `ml_testset` as the argument. Name your model object `collaborative_filter_model`.  \n",
    "> 2. Compute the recommender predictions with the `test` method using the `ml_testset` as the argument.   \n",
    "> 3. Compute and print the RMSE using `surprise.accuracy.msee` and MAE using `surprise.accuracy.mae` with the predictions as the argument in both cases.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5acf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate the model object and fit to the training data  \n",
    "## Put your code here\n",
    "\n",
    "\n",
    "## Compute the predicitons from the model \n",
    "## Put your code here\n",
    "\n",
    "\n",
    "# Compute some model performance statistics\n",
    "## Put your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cd2919",
   "metadata": {},
   "source": [
    "> 4. Compare the performance of this model to the baseline model. Is it better or worse? Keeping in mind that the rating sampling is more dense that a real-world example, what does this tell you about the effectiveness of a baseline model vs. the model based on latent factors.    \n",
    "> 5. The distribution of the item and factor values gives insight into the behavior of the model. Execute the code in the to display the histograms of the factors for items and users.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd4f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of item factor matrix: {}'.format(svd_model.qi.shape))\n",
    "print('Shape of user factor matrix: {}'.format(svd_model.pu.shape))\n",
    "\n",
    "## Create a histograms of the factor values\n",
    "fig,ax= plt.subplots(1,2,figsize=(15,4))\n",
    "_=ax[0].hist(svd_model.qi.flatten(), bins=20, alpha=0.5) \n",
    "_=ax[0].set_xlabel('Item factor values')\n",
    "_=ax[0].set_ylabel('Frequency')\n",
    "_=ax[0].set_title('Distribution of item factor values')\n",
    "_=ax[0].set_xlim(-.75,.75)\n",
    "_=ax[1].hist(svd_model.pu.flatten(), bins=20, alpha=0.5) \n",
    "_=ax[1].set_xlabel('User factor values')\n",
    "_=ax[1].set_ylabel('Frequency')\n",
    "_=ax[1].set_title('Distribution of user factor values')\n",
    "_=ax[1].set_xlim(-.75,.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e046cf72",
   "metadata": {},
   "source": [
    "> 6. Notice the compact representation of the factors computed. Do you expect this representation to be compact compared to the original data set?     \n",
    "> 7. Examine the distribution of the factor values. Compare these values to the distribution of bias values for the Baseline model. What does the fact that these ranges are small compared to the range of bias values tell you about the effect these values will have on the predictions of the latent variable model?     \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfda811",
   "metadata": {},
   "source": [
    "#### Copyright 2021, Stephen F Elston. All rights reserved. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
