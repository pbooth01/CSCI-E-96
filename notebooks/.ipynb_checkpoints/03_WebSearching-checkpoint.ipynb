{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 03\n",
    "## Web Search\n",
    "## CSCI S-96    \n",
    "\n",
    "> **Instructions:** For this assignment you will complete the exercises shown. All exercises involve creating and executing some Python code. Additionally, most exercises have questions for you to answer. You can answer questions by creating a Markdown cell and writing your answer. If you are not familiar with Markdown, you can find a brief tutorial [here](https://www.markdownguide.org/cheat-sheet/).     \n",
    "\n",
    "In this assignment you will gain some experience and insight into how web search algorithms work. Specifically, you will implement versions of three algorithms, simple PageRank, PageRank with damping, and the HITS algorithm. All three of these algorithms use a directed graph model of the web.   \n",
    "\n",
    "Data small scale examples and coding methods used here are not directly scalable to web sized problems. Rather, the point is for you to understand the basic characteristics of these web search algorithms. Web scale searching requires massive resources not readily available to most people. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple PageRank Example\n",
    "\n",
    "To get a feeling for the basics of the PageRank algorithm you will create and test simple code. \n",
    "\n",
    "As a first step, execute the code in the cell below to import the packages you will need for the exercises. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a simple example. Figure 1 shows a set of web pages and their hyperlinks. This is a directed graph with the pages as nodes and the hyperlinks as the directed edges. This graph is **complete**. Every page is accessable from any other page, possibly with visits to intermediate nodes required.  \n",
    "\n",
    "<img src=\"../images/Web1.png\" alt=\"Drawing\" style=\"width:500px; height:400px\"/>\n",
    "<center>Figure 1: A small set of web pages</center>\n",
    "\n",
    "The directed edges of the graph define the association between the nodes. A directed edge, or hyperlink, runs from a node's column to another node's row. The association is binary. The directed edge either exists or it does not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 03-1:** In the cell below you will create the association matrix and the initial page probability vector. Do the following:  \n",
    "> 1. Create the association matrix, $A$, using [numpy.array](https://numpy.org/doc/stable/reference/generated/numpy.array.html). This matrix is constructed with a 1 where a page in a column links to another page, and 0 elsewhere.  \n",
    "> 2. Print the shape of your association matrix as a check of your association matrix.\n",
    "> 3. Print the in degree and out degree of your association matrix, using [numpy.sum](https://numpy.org/doc/stable/reference/generated/numpy.sum.html). Set the argument `axis` to 1 to sum across rows and 0 to sum down columns. \n",
    "> 3. Create the uniformly distributed page probability vector, $p$, using `numpy.array`. \n",
    "> 4. Verify that the page probability vector sums to 1.0 using `numpy.sum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the association matrix is:  (5, 5)\n",
      "The out degree is:  [3 3 2 2 1]\n",
      "The in degree is:  [2 1 1 4 3]\n",
      "The sum of the page probability vector is:  1.0\n"
     ]
    }
   ],
   "source": [
    "## Create the Association Matrix and print the summary\n",
    "A = np.array(\n",
    "    [[0, 1, 0, 1, 0],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [1, 1, 1, 0, 1],\n",
    "    [0, 1, 1, 1, 0]]\n",
    ")\n",
    "\n",
    "print(\"The shape of the association matrix is: \", A.shape)\n",
    "print(\"The out degree is: \", np.sum(A, axis=0))\n",
    "print(\"The in degree is: \", np.sum(A, axis=1))\n",
    "\n",
    "\n",
    "## Create the equal probability starting values\n",
    "p = np.array([1/5, 1/5, 1/5, 1/5, 1/5])\n",
    "print(\"The sum of the page probability vector is: \", np.sum(p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer the following questions:  \n",
    "> - Are the out degree and in degree you computed from the association matrix consistent with the graph? \n",
    "> - Are the page probabilities a proper distribution summing to 1.0? \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes both the In degree and Out degree are consistent with the graph. Yes the page probabilities sum up to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the PageRank algorithm we must normalize the association matrix by the inverse of the out degree of each page. This normalization uniformly distributes the influence of each page on the PageRank of the other pages over its out degree. The calculation of the out degree of a network is illustrated in Figure 2.  \n",
    "\n",
    "<img src=\"../images/outdegree.png\" alt=\"Drawing\" style=\"width:400px; height:200px\"/>\n",
    "<center>Figure 2: Example of computing out degree from an association matrix</center>\n",
    "\n",
    "The normalized transition probability matrix, $M$, is then computed from the association matrix, $A$: \n",
    "\n",
    "$$M = A D^{-1}$$\n",
    "\n",
    "Where, $D^{-1}$ is the inverse of a matrix with the out degree values on the diagonal and zeros elsewhere.  \n",
    "\n",
    "You can see from the foregoing that $M$ distributes the influence of the page by the in|verse of the out degree. In other words, the influence is inversely weighted by the number of pages each page links to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 03-2:** You will now compute the normalized transition matrix, $M$. To do so create a function called `norm_association` with the association matrix as the argument. Do the following: \n",
    "> 1. Create your function `norm_association` which will do the following:  \n",
    ">    - Compute the sum of the columns of the association matrix using `numpy.sum` with the `axis=0` argument to sum along columns. \n",
    ">   - Compute the inverse of the column sums as a vector. Be sure to avoid zero divides, which will occur in subsequent exercises. If the column sum is 0 set the inverse to zero.   \n",
    ">   - Create a square diagonal matrix from the inverse column sums using [numpy.diag](https://numpy.org/doc/stable/reference/generated/numpy.diag.html) to form the inverse out degree diagonal matrix.\n",
    ">  - Finally, return the matrix product of the association matrix and inverse out degree matrix using [numpy.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).  \n",
    "2. Execute your function on the association matrix you computed. Save and print the normalized transition matrix, and examine the result.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.33333333 0.         0.5        0.        ]\n",
      " [0.33333333 0.         0.         0.         0.        ]\n",
      " [0.33333333 0.         0.         0.         0.        ]\n",
      " [0.33333333 0.33333333 0.5        0.         1.        ]\n",
      " [0.         0.33333333 0.5        0.5        0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def inverter(n):\n",
    "    if n == 0:\n",
    "        return n\n",
    "    else:\n",
    "        return 1 / n\n",
    "\n",
    "def norm_association(A):\n",
    "    '''Function to normalize the association matrix by out degree.\n",
    "    The function accounts for cases where the column sum is 0'''\n",
    "    out_degree = np.sum(A, axis=0)\n",
    "    D_inv = np.diag(list(map(inverter, out_degree)))\n",
    "    return np.matmul(A, D_inv)\n",
    "\n",
    "\n",
    "## Execute the function\n",
    "M = norm_association(A)\n",
    "\n",
    "print(M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Does the result look correct based on the out degree of each page on the graph?  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the transition probability matrix, $M$ computed it is time to investigate the convergence of the PageRank algorithm. You can think of the PageRank algorithm as a series of transitions of a Markov Chain. Given the transition probability matrix, $M$, the update, or single Markov transition, of the page probabilities, $p_i$, is computed: \n",
    "\n",
    "$$p_i = M p_{i-1}$$\n",
    "\n",
    "The Markov chain can be executed for a great many transitions. The result of $n$ transitions, starting from an initial set of page probabilities, $p_0$, can be written:  \n",
    "\n",
    "$$p_n = M^n p_{0}$$\n",
    "\n",
    "At convergence the page probabilities, $p_n$, approach a constant or **steady state** value. This steady state probability vector values are the PageRank of the web pages.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 03-3:** You will now create and execute code with the goal of getting a feel for how the page probabilities change for a single transition of a Markov process. The accomplish this task you will create a function called `transition` with arguments of the the normalized transition probability matrix and the vector of page probabilities. Specifically you will:   \n",
    "> 1. Create the function `transition` which uses [numpy.dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) to compute the product of the transition matrix and the page probability vector.  \n",
    "> 2. Execute the `transition` function on the normalized transition probability matrix and vector of initial page probabilities you have created, saving the result. \n",
    "> 3. Print the Euclidean (L2) norm of the difference between the initial page probabilities and the updated page probabilities. \n",
    "> 4. Print the sum of the page probabilities computed with `transition` along with the actual vector of values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The L2 norm of the p0 and p1 page probabilities is:  0.30912061651652345\n",
      "The sum of the new page probabilities is:  1.0\n",
      "p_1:  [0.16666667 0.06666667 0.06666667 0.43333333 0.26666667]\n"
     ]
    }
   ],
   "source": [
    "def transition(transition_probs, probs):\n",
    "    '''Function to compute the probabilities resulting from a \n",
    "    single transition of a Markov process'''\n",
    "    return np.dot(transition_probs, probs)\n",
    "\n",
    "## Compute probabilities after first state trasnition and print the sumamries\n",
    "p_1 = transition(M, p)\n",
    "euclidean_distance = np.linalg.norm(p-p_1)\n",
    "\n",
    "print(\"The L2 norm of the p0 and p1 page probabilities is: \", euclidean_distance)\n",
    "print(\"The sum of the new page probabilities is: \", np.sum(p_1))\n",
    "print(\"p_1: \", p_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Is the sum of the page probabilities equal to 1.0 as it should be? Considering in degree of the pages, are the relative changes in the page probabilities what you would expect and why.  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes the sume of the new page probabilities is equal to 1.0. Yes the relative changes in page probabilities is what I expected. The page with the largest out degree experienced that greatest increase and pages that had the same out degree experienced that same changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 03-4:** You will continue with computing transitions of the Markov chain. Use the `transition` function with the normalized transition probability matrix and the page probability vector computed from the first transition as arguments. Your code must do the following:  \n",
    "> 1. Compute the resulting page probabilities of the second transition.\n",
    "> 2. Compute and print the Euclidean (L2) norm of the difference between the page probabilities before and after the transition. \n",
    "> 3. Compute and print the sum of the page probabilities. \n",
    "> 4. Display the page probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The L2 norm of the p1 and p2 page probabilities is:  0.09262962222518371\n",
      "The sum of the new page probabilities is:  1.0\n",
      "p_2:  [0.23888889 0.05555556 0.05555556 0.37777778 0.27222222]\n"
     ]
    }
   ],
   "source": [
    "# Compute probabilities after second state transition\n",
    "p_2 = transition(M, p_1)\n",
    "euclidean_distance = np.linalg.norm(p_1-p_2)\n",
    "\n",
    "print(\"The L2 norm of the p1 and p2 page probabilities is: \", euclidean_distance)\n",
    "print(\"The sum of the new page probabilities is: \", np.sum(p_2))\n",
    "print(\"p_2: \", p_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note the difference between the Euclidean norms of the differences for the first and second transition calculations. Does the change in this difference from one step to the next indicate the algorithm is converging to the steady state probabilities?  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes the euclidean norm of the difference is 3 times smaller than it was previously showing that the algorithm seems to be converging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 03-4:** The question now is how does this simplified version of page rank converge with more iterations? To find out, do the following:   \n",
    "> 1. Create a function `pagerank1` with arguments, the normalized transition matrix, the initial page probabilities and a convergence threshold value of 0.01, which does the following:  \n",
    ">    - Initialize a euclidean distance norm to 1.0 and the resulting page probabilities to a vector of 0.0 values of length equal to the dimension of the transition matrix.   \n",
    ">    - Set a loop counter to 1.  \n",
    ">    - Use a 'while' loop with termination conditions the euclidean distance norm less than the threshold value AND the loop counter less than 50.  In side this loop do the following:  \n",
    ">      1. Update the page probabilities using the `transition` function you created. \n",
    ">      2. Compute the Euclidean norm of the difference between the initial and updated page probabilities from the transition.   \n",
    ">      3. Print the value of the loop counter and the Euclidean norm value. \n",
    ">      4. Copy the updated page probability vector into the input page probability vector.  \n",
    ">      5. Increment the loop counter by 1. \n",
    ">    - Return the page probabilities at convergence.  \n",
    "> 2. Execute your `pagerank1` function using the transition matrix and initial page probability vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iteration:  1\n",
      "Current Euclidean Distance:  0.30912061651652345\n",
      "************************\n",
      "Current Iteration:  2\n",
      "Current Euclidean Distance:  0.09262962222518371\n",
      "************************\n",
      "Current Iteration:  3\n",
      "Current Euclidean Distance:  0.0627447198003608\n",
      "************************\n",
      "Current Iteration:  4\n",
      "Current Euclidean Distance:  0.04713034716117629\n",
      "************************\n",
      "Current Iteration:  5\n",
      "Current Euclidean Distance:  0.04044983066499079\n",
      "************************\n",
      "Current Iteration:  6\n",
      "Current Euclidean Distance:  0.034751469502374295\n",
      "************************\n",
      "Current Iteration:  7\n",
      "Current Euclidean Distance:  0.02965169022900917\n",
      "************************\n",
      "Current Iteration:  8\n",
      "Current Euclidean Distance:  0.025297173726972686\n",
      "************************\n",
      "Current Iteration:  9\n",
      "Current Euclidean Distance:  0.021602992445622173\n",
      "************************\n",
      "Current Iteration:  10\n",
      "Current Euclidean Distance:  0.018452365792341184\n",
      "************************\n",
      "Current Iteration:  11\n",
      "Current Euclidean Distance:  0.01575978617875431\n",
      "************************\n",
      "Current Iteration:  12\n",
      "Current Euclidean Distance:  0.013459509556315313\n",
      "************************\n",
      "Current Iteration:  13\n",
      "Current Euclidean Distance:  0.011495031885715244\n",
      "************************\n",
      "Current Iteration:  14\n",
      "Current Euclidean Distance:  0.00981734526591825\n",
      "************************\n",
      "Converged Page Probabilities:  [0.21890828 0.07149265 0.07149265 0.38258726 0.25551917]\n",
      "Sum of Converged PRobabilities:  0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "# Compute probabilities after a larger number of state transitions \n",
    "def pagerank1(M, in_probs,  threshold = 0.01):  \n",
    "    euclidean_dist = 1.0 \n",
    "    page_probabilities = np.array([0.0]*len(M))\n",
    "    i = 1   \n",
    "    while euclidean_dist >= threshold and i < 50:\n",
    "        page_probabilities = transition(M, in_probs)\n",
    "        euclidean_dist = np.linalg.norm(in_probs-page_probabilities)\n",
    "        \n",
    "        print(\"Current Iteration: \", i)\n",
    "        print(\"Current Euclidean Distance: \", euclidean_dist)\n",
    "        print(\"************************\")\n",
    "        \n",
    "        in_probs = page_probabilities\n",
    "        \n",
    "        i = i + 1\n",
    "    \n",
    "    return page_probabilities\n",
    "        \n",
    "    \n",
    "\n",
    "## Execute your function\n",
    "converged_page_probabilities = pagerank1(M, p)\n",
    "\n",
    "print(\"Converged Page Probabilities: \", converged_page_probabilities)\n",
    "print(\"Sum of Converged PRobabilities: \", np.sum(converged_page_probabilities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer the following questions:  \n",
    "> - Does the convergence of this algorithm seem fairly rapid? \n",
    "> - Does the rank order of the computed page probabilities make sense given the directed graph of the pages? \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes the convergence seems pretty rapid. Yes the rank order makes sense. It stays the same, based on out degree, after convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A More Complicated Example   \n",
    "\n",
    "You will now work with a more complicated example The graph of 6 web pages, shown in Figure 2, is no longer complete. The out degree of page 6 is 0. A random surfer transitioning to page 6 will have no escape, a **spider trap**! \n",
    "\n",
    "<img src=\"../images/Web2.png\" alt=\"Drawing\" style=\"width:500px; height:500px\"/>\n",
    "<center>Figure 3: A small set of web pages with a dead end</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 03-5:** You will now create both the normalized transition matrix and the initial page probability vector for the graph of Figure 3. In this exercise you will . Do the following:  \n",
    "> 1. Create the association matrix and normalize it using your `norm_association` function. Name your transition matrix M_deadend. Print the result. \n",
    "> 2. Create a vector of containing the the in degree of the graph. Normalize the vector so the initial probabilities sum to 1.0. Save and print the result.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the association matrix is:  (6, 6)\n",
      "The out degree is:  [4 3 3 2 1 0]\n",
      "The in degree is:  [2 1 1 4 3 2]\n",
      "[[0.         0.33333333 0.         0.5        0.         0.        ]\n",
      " [0.25       0.         0.         0.         0.         0.        ]\n",
      " [0.25       0.         0.         0.         0.         0.        ]\n",
      " [0.25       0.33333333 0.33333333 0.         1.         0.        ]\n",
      " [0.         0.33333333 0.33333333 0.5        0.         0.        ]\n",
      " [0.25       0.         0.33333333 0.         0.         0.        ]]\n",
      "The sum of the page probability vector is:  0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "A_deadend = np.array(\n",
    "    [[0, 1, 0, 1, 0, 0],\n",
    "    [1, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0, 0],\n",
    "    [1, 1, 1, 0, 1, 0],\n",
    "    [0, 1, 1, 1, 0, 0],\n",
    "    [1, 0, 1, 0, 0, 0]]\n",
    ")\n",
    "\n",
    "print(\"The shape of the association matrix is: \", A_deadend.shape)\n",
    "print(\"The out degree is: \", np.sum(A_deadend, axis=0))\n",
    "print(\"The in degree is: \", np.sum(A_deadend, axis=1))\n",
    "\n",
    "## Normalize the association matrix\n",
    "M_deadend = norm_association(A_deadend)\n",
    "\n",
    "print(M_deadend)\n",
    "\n",
    "## Create the equal probability starting values\n",
    "p_deadend = np.array([1/6, 1/6, 1/6, 1/6, 1/6, 1/6])\n",
    "print(\"The sum of the page probability vector is: \", np.sum(p_deadend))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "       0.16666667])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_deadend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degree = np.sum(A_deadend, axis=1)\n",
    "in_degree\n",
    "total_in_degree = np.sum(in_degree)\n",
    "\n",
    "def in_degree_inverter(n):\n",
    "    if n == 0:\n",
    "        return n\n",
    "    else:\n",
    "        return n / total_in_degree\n",
    "\n",
    "inversed_in_degree = np.array(list(map(in_degree_inverter, in_degree)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine your results. Are the 0 values for the transition probabilities of page 6 and the associated 0 in the page probabilities consistent with the graph of these pages?    \n",
    "> **End of exercise.**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes they are consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercises 03-6:** What happens if you apply the simplified PageRank algorithm to the pages on a graph that is not complete, like the one shown in Figure 2? To find out, execute your `pagerank1` function with arguments `M_deadend`, `p_deadend` and `threshold=0.001`. The smaller threshold value is to ensure convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iteration:  1\n",
      "Current Euclidean Distance:  0.24689428936987745\n",
      "************************\n",
      "Current Iteration:  2\n",
      "Current Euclidean Distance:  0.087290313124837\n",
      "************************\n",
      "Current Iteration:  3\n",
      "Current Euclidean Distance:  0.05112283061425409\n",
      "************************\n",
      "Current Iteration:  4\n",
      "Current Euclidean Distance:  0.040902112934503264\n",
      "************************\n",
      "Current Iteration:  5\n",
      "Current Euclidean Distance:  0.03330965740599665\n",
      "************************\n",
      "Current Iteration:  6\n",
      "Current Euclidean Distance:  0.0315625664858363\n",
      "************************\n",
      "Current Iteration:  7\n",
      "Current Euclidean Distance:  0.02559781956037353\n",
      "************************\n",
      "Current Iteration:  8\n",
      "Current Euclidean Distance:  0.024309802780145634\n",
      "************************\n",
      "Current Iteration:  9\n",
      "Current Euclidean Distance:  0.020108336170986056\n",
      "************************\n",
      "Current Iteration:  10\n",
      "Current Euclidean Distance:  0.019211686938674855\n",
      "************************\n",
      "Current Iteration:  11\n",
      "Current Euclidean Distance:  0.016185076011934068\n",
      "************************\n",
      "Current Iteration:  12\n",
      "Current Euclidean Distance:  0.015480019371330162\n",
      "************************\n",
      "Current Iteration:  13\n",
      "Current Euclidean Distance:  0.01326780749245994\n",
      "************************\n",
      "Current Iteration:  14\n",
      "Current Euclidean Distance:  0.012663894730572493\n",
      "************************\n",
      "Current Iteration:  15\n",
      "Current Euclidean Distance:  0.011017095534036453\n",
      "************************\n",
      "Current Iteration:  16\n",
      "Current Euclidean Distance:  0.0104759911826907\n",
      "************************\n",
      "Current Iteration:  17\n",
      "Current Euclidean Distance:  0.009226369237005644\n",
      "************************\n",
      "Current Iteration:  18\n",
      "Current Euclidean Distance:  0.008734354786517063\n",
      "************************\n",
      "Current Iteration:  19\n",
      "Current Euclidean Distance:  0.007768416344424052\n",
      "************************\n",
      "Current Iteration:  20\n",
      "Current Euclidean Distance:  0.00732165412469536\n",
      "************************\n",
      "Current Iteration:  21\n",
      "Current Euclidean Distance:  0.006562380307895495\n",
      "************************\n",
      "Current Iteration:  22\n",
      "Current Euclidean Distance:  0.006159913350301158\n",
      "************************\n",
      "Current Iteration:  23\n",
      "Current Euclidean Distance:  0.0055543616131291645\n",
      "************************\n",
      "Current Iteration:  24\n",
      "Current Euclidean Distance:  0.005195271679900301\n",
      "************************\n",
      "Current Iteration:  25\n",
      "Current Euclidean Distance:  0.0047064012956031746\n",
      "************************\n",
      "Current Iteration:  26\n",
      "Current Euclidean Distance:  0.004388951992987616\n",
      "************************\n",
      "Current Iteration:  27\n",
      "Current Euclidean Distance:  0.00399031384946938\n",
      "************************\n",
      "Current Iteration:  28\n",
      "Current Euclidean Distance:  0.003711925744238664\n",
      "************************\n",
      "Current Iteration:  29\n",
      "Current Euclidean Distance:  0.003384225395864782\n",
      "************************\n",
      "Current Iteration:  30\n",
      "Current Euclidean Distance:  0.003141728375935038\n",
      "************************\n",
      "Current Iteration:  31\n",
      "Current Euclidean Distance:  0.0028705912118335256\n",
      "************************\n",
      "Current Iteration:  32\n",
      "Current Euclidean Distance:  0.0026605144569068185\n",
      "************************\n",
      "Current Iteration:  33\n",
      "Current Euclidean Distance:  0.002435017209915368\n",
      "************************\n",
      "Current Iteration:  34\n",
      "Current Euclidean Distance:  0.00225382940808235\n",
      "************************\n",
      "Current Iteration:  35\n",
      "Current Euclidean Distance:  0.0020655211357826753\n",
      "************************\n",
      "Current Iteration:  36\n",
      "Current Euclidean Distance:  0.0019098010349052166\n",
      "************************\n",
      "Current Iteration:  37\n",
      "Current Euclidean Distance:  0.0017520391506812042\n",
      "************************\n",
      "Current Iteration:  38\n",
      "Current Euclidean Distance:  0.0016185828686854043\n",
      "************************\n",
      "Current Iteration:  39\n",
      "Current Euclidean Distance:  0.0014860743190327323\n",
      "************************\n",
      "Current Iteration:  40\n",
      "Current Euclidean Distance:  0.0013719535875059037\n",
      "************************\n",
      "Current Iteration:  41\n",
      "Current Euclidean Distance:  0.0012604315181295625\n",
      "************************\n",
      "Current Iteration:  42\n",
      "Current Euclidean Distance:  0.0011630171484023626\n",
      "************************\n",
      "Current Iteration:  43\n",
      "Current Euclidean Distance:  0.0010690084872142743\n",
      "************************\n",
      "Current Iteration:  44\n",
      "Current Euclidean Distance:  0.0009859708571692976\n",
      "************************\n",
      "Converged Dead End Page Probabilities:  [0.00500526 0.00135769 0.00135769 0.00830595 0.0054972  0.00184963]\n"
     ]
    }
   ],
   "source": [
    "converged_page_de_probabilities = pagerank1(M_deadend, p_deadend, threshold=0.001)\n",
    "\n",
    "print(\"Converged Dead End Page Probabilities: \", converged_page_de_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer the following questions:  \n",
    "> - Given the lower convergence threshold (factor of 10) does the convergence of the algorithm generally seem fast? \n",
    "> - Examine the page probabilities, noticing that the sum is far less than 1.0. Does this indicate that the algorithm is converging to a point where all page probabilities are 0.0?  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convergence still seems faily fast since lowering the convergence threshold by a factor of 10 did not result in the convergence taking 10 times longer to occur. Yes this indicates that the algorithm is making the page probability vector converge to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the results of the foregoing exercise that the simple PageRank algorithm does not converge to a usable set of page probabilities when faced with graph that is not complete.Fortunately, there is a simple fix, add a damping term. You can think of the damping term as allowing a random surfer to make an arbitrary transition or jump with some small probability. These random jumps help the random surfer to better explore the graph and to escape from spider traps. The jump probabilities from states, $p_i$, are a function of the damping factor $d$: \n",
    "\n",
    "$$Jump\\ Probability = \\frac{(1-d)}{n} p_i$$\n",
    "\n",
    "Where $n$ is the dimension of the transition probability matrix. \n",
    "\n",
    "The updated page probabilities, $p_i$, are then computed with the damped PageRank algorithm as:   \n",
    "\n",
    "$$p_{i} = d * M p_{i-1} + \\frac{(1-d)}{n} p_{i-1}$$\n",
    "\n",
    "Where $M$ is the transition probability matrix and p are the initial page probability values. \n",
    "\n",
    "> **Exercise 03-7:** To implement the PageRank algorithm with a damping factor do the following:  \n",
    "> 1. Create a `transition_damped` function with arguments, the transition probability matrix, the initial page probabilities, and the damping factor, $d=0.85$, which does the following:  \n",
    ">   - Compute the updated page probabilities by multiplying the inner (dot) product of the transition probability matrix by the initial page probabilities by the damping factor, `d`. Use the [numpy.multiply](https://numpy.org/doc/stable/reference/generated/numpy.multiply.html) and numpy.dot functions.\n",
    ">   - Compute the jump probabilities vector of length the dimension of the transition matrix and multiply these by element wise by the page probabilities using `np.multiply`. *Note:* the jump probabilities are constant, so you can create code that only computes them once if you so choose.  \n",
    ">   - Return the sum of the damped page probabilities and the jump probabilities.  \n",
    "> 2. Create a `pagerank_damped` function. This function is identical to the `pagerank1` function you already created except that it uses the `transiton_damped` function in place of the `transition` function.  \n",
    "> 3. Call your `pagerank_damped` function using arguments of `M_deadend` and `p_deadend`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iteration:  1\n",
      "Current Euclidean Distance:  0.20986014596439584\n",
      "************************\n",
      "Current Iteration:  2\n",
      "Current Euclidean Distance:  0.06306725123269477\n",
      "************************\n",
      "Current Iteration:  3\n",
      "Current Euclidean Distance:  0.03139580835097885\n",
      "************************\n",
      "Current Iteration:  4\n",
      "Current Euclidean Distance:  0.021351158590016534\n",
      "************************\n",
      "Current Iteration:  5\n",
      "Current Euclidean Distance:  0.014779671948595703\n",
      "************************\n",
      "Current Iteration:  6\n",
      "Current Euclidean Distance:  0.011903806662015098\n",
      "************************\n",
      "Current Iteration:  7\n",
      "Current Euclidean Distance:  0.00820607446101341\n",
      "************************\n",
      "Converged Damped Page Probabilities:  [0.13296881 0.05440639 0.05440639 0.21648663 0.14839135 0.06982893]\n",
      "Sum of Converged Page Rank:  0.6764884940142689\n"
     ]
    }
   ],
   "source": [
    "## Add a damping facgtor to the transiton \n",
    "def transition_damped(transition_probs, probs, d=0.85):\n",
    "    '''Function to compute the probabilities resulting from a \n",
    "    single transition of a Markov process including a damping\n",
    "    factor to deal with dead ends'''\n",
    "    n = len(transition_probs)\n",
    "    \n",
    "    undamped_probs = np.dot(transition_probs, probs)\n",
    "    damped_probs = np.multiply(undamped_probs, d)\n",
    "    \n",
    "    jp = ((1 - d) / n)\n",
    "    \n",
    "    return (damped_probs + jp)\n",
    "\n",
    "\n",
    "## function for the PageRank algorithm using the damped transition algorithm \n",
    "def pagerank_damped(M, in_probs,  threshold = 0.01):  \n",
    "    euclidean_dist = 1.0 \n",
    "    page_probabilities = np.array([0.0]*len(M))\n",
    "    i = 1   \n",
    "    while euclidean_dist >= threshold and i < 50:\n",
    "        page_probabilities = transition_damped(M, in_probs)\n",
    "        euclidean_dist = np.linalg.norm(in_probs-page_probabilities)\n",
    "        \n",
    "        print(\"Current Iteration: \", i)\n",
    "        print(\"Current Euclidean Distance: \", euclidean_dist)\n",
    "        print(\"************************\")\n",
    "        \n",
    "        in_probs = page_probabilities\n",
    "        \n",
    "        i = i + 1\n",
    "    \n",
    "    return page_probabilities\n",
    "\n",
    "\n",
    "## Execute your funciton\n",
    "converged_damped_page_probabilities = pagerank_damped(M_deadend, p_deadend)\n",
    "\n",
    "print(\"Converged Damped Page Probabilities: \", converged_damped_page_probabilities)\n",
    "\n",
    "print(\"Sum of Converged Page Rank: \", np.sum(converged_damped_page_probabilities))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer the following questions:   \n",
    "> - Notice the convergence of the damped page rank algorithm. Compare this convergence to that of the undamped PageRank algorithm on the complete graph of 5 pages. Do you think this change is a result of the jumps the random surfer makes?   \n",
    "> - Examine the final page probabilities. Does the rank of these page probabilities make sense given the graph of the pages in this case? \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convergence of the damped page rank algorithm is .67 where the convergence of the undamped pagerank algorithm on the Complete graph was .99. Yes I think this result is caused by a mixture of the random jumps and also the fact that page 6 has all zeros in the association matrix.\n",
    "\n",
    "Yes the rank of the pages makes sense based on the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hubs, Authorities, and the HITS Algorithm  \n",
    "\n",
    "The hubs and authorities model is an alternative to PageRank. Rather than using a single metric to rank the importance of web pages, the **HITS** algorithm iteratively updates the hub and authority scores for each of the pages. \n",
    "\n",
    "The HITS algorithm updates the authority and hub scores iteratively. The authority score is sum of the hubs linked to it: \n",
    "$$ð‘Ž= \\beta ð´ â„Ž$$\n",
    "\n",
    "Hub score is sum of the authorities it links to: \n",
    "$$â„Ž= \\alpha ð´^ð‘‡ a$$\n",
    "\n",
    "The algorithm iterates between updates to $ð‘Ž$ and $â„Ž$ until convergence. To ensure convergence, must normalize $ð‘Ž$ and $â„Ž$ to have unit Euclidean norm at each iteration. Therefore, the choice of $\\alpha$ and $\\beta$ are therefore unimportant      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ** Exercise 03-8:** To understand the HITS algorithm you will now create and test code for this algorithm. Follow these steps:  \n",
    "> 1. Create a function called `HITS` with arguments of the association matrix, initial hub vector, initial authority vector, and the number of iterations of the algorithm to run. This function does the following:   \n",
    ">  - Inside a for loop over the number of iterations:  \n",
    ">    1. Updates the authority vector using the association matrix and the hub vector as argument to the `transition` function. \n",
    ">    2. Normalizes the authority vector by using `numpy.divide` with arguments of the updated authority vector and its L2 norm, computed with [numpy.linalg.norm](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html).  \n",
    ">    3. Updates the hub vector using the association matrix and the authority vector as argument to the `transition` function. \n",
    ">    4. Normalizes the hub vector by using `numpy.divide` with arguments of the updated hub vector and its L2 norm, computed with `numpy.linalg.norm`.  \n",
    ">   - Return the hub and authority vectors\n",
    "> 2. Initialize an initial hub and authority vector of length the dimension of the association matrix with uniformly distributed values of $\\frac{1.0}{dimension(association\\ matrix)}$.  \n",
    "> 3. Execute your function using the association matrix for the 6-page network and the initial hub and authority vectors as arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Hub Score:  [0.24657989 0.39765758 0.38616873 0.21363502 0.76570268 0.        ]\n",
      "Calculated Authority Score:  [0.20075256 0.05169981 0.05169981 0.91299596 0.30870866 0.15965591]\n"
     ]
    }
   ],
   "source": [
    "def HITS(association, hub, authority, iters=100):\n",
    "    for i in range(iters):\n",
    "        authority = transition(association, hub)\n",
    "        authority = np.divide(authority, np.linalg.norm(authority))\n",
    "        \n",
    "        hub = transition(association.transpose(), authority)\n",
    "        hub = np.divide(hub, np.linalg.norm(hub))\n",
    "    \n",
    "    return hub, authority\n",
    "\n",
    "\n",
    "## Compute the intial hub and authority vectors\n",
    "## Put your code below\n",
    "alpha = np.array([1/6, 1/6, 1/6, 1/6, 1/6, 1/6])\n",
    "beta = np.array([1/6, 1/6, 1/6, 1/6, 1/6, 1/6])\n",
    "\n",
    "\n",
    "## Execute your funciton\n",
    "calculated_hub, calculated_authority = HITS(M_deadend, alpha, beta)\n",
    "\n",
    "print(\"Calculated Hub Score: \", calculated_hub)\n",
    "print(\"Calculated Authority Score: \", calculated_authority)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine your results and answer the following questions:\n",
    "> - Which of the pages have the highest hub scores? Considering the graph of the pages, is this ordering consistent?  \n",
    "> - Notice the last value of the hub scores. Is this value expected given the graph of the pages? \n",
    "> - Which of the pages have the highest authority. Given the in degree of the pages is this ranking consistent?  \n",
    "> - Compare the ranking of the pages based on authority that found with PageRank. Are these results consistent? \n",
    "> **End of exercise.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page 5 has the highest Hub score. Yes this makes sense given the graph because page 5 links to page 4 which has the highest in-degree and thus is the highest authority.\n",
    "\n",
    "Yes the fact that page 6 has a hub score of 0 is expected since it has an out degree of 0.\n",
    "\n",
    "Page 4 is the highest authority and has the highest in degree.\n",
    "\n",
    "Yes the rankings are consistent between HITS and pagerank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2021, Stephen F Elston. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
