{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc9df8b5",
   "metadata": {},
   "source": [
    "# Assignment 04\n",
    "\n",
    "## Dimensionality Reduction \n",
    "\n",
    "## CSCI S-96\n",
    "\n",
    "> Instructions: For this assignment you will complete the exercises shown. All exercises involve creating and executing some Python code. Additionally, most exercises have questions for you to answer. You can answer questions by creating a Markdown cell and writing your answer. If you are not familiar with Markdown, you can find a brief tutorial here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b59b65",
   "metadata": {},
   "source": [
    "## Introduction  \n",
    "\n",
    "Dimensionality reduction algorithms are widely used in data mining. Human perception of relationships in data is limited beyond a few dimensions. Further, many data mining algorithms produce poor results where when there is significant dependency between the features. In both cases, we can apply dimensionality reduction methods.   \n",
    "\n",
    "In the exercises in this notebook you will gain some experience working with some commonly used dimensionality reduction methods. Specifically, there are two distinct classes of algorithms you will explore:    \n",
    "1. **Dimensionality reduction transformation methods** create operators to map a sample space to an orthogonal space. Typically the original data can be well-represented in lower dimensions in the orthogonal space. Examples of these methods include principle component analysis (PCA) and kernel principle component analysis (Kernal PCA).    \n",
    "2. **Manifold learning methods** where data in a high dimensional space is mapped onto a 2-dimensional manifold. Manifold learning is primarily used to aid visualization of high-dimensional data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a5ad06",
   "metadata": {},
   "source": [
    "To begin, execute the code in the cell below to import the packages you will need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3105ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import SpectralEmbedding, MDS\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c15c139",
   "metadata": {},
   "source": [
    "## A Synthetic Example\n",
    "\n",
    "To make the ideas of dimensionality reduction clear, we start with an extremely simple example. In this example, dimensionality reduction is applied to bivariate Normally distributed data. The code in the cell below does the following:  \n",
    "1. Generate 500 samples from a bivariate, zero-centered Normal distribution with covariance having a high degree of dependency between the variables:  \n",
    "$$\n",
    "cov = \n",
    " \\begin{bmatrix}\n",
    "   1.0 & 0.9\\\\\n",
    "   0.9 & 1.0\n",
    "   \\end{bmatrix}\n",
    "$$\n",
    "2. Print the empirical covariance matrix of the sample.  \n",
    "3. Plot the simulated data values.\n",
    "\n",
    "Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e3149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normal(X):\n",
    "    X = pd.DataFrame(X, columns=['axis1','axis2'])\n",
    "    _=sns.jointplot(x='axis1', y='axis2', data=X, xlim=(-4.5,4.5), ylim=(-4.5,4.5))\n",
    "\n",
    "cov = [[1, 0.9], [0.9, 1]]\n",
    "np.random.seed(367)\n",
    "Normal_random = np.random.multivariate_normal([0.0,0.0], cov, size=500)\n",
    "print(np.cov(Normal_random[:,0], Normal_random[:,1]))\n",
    "plot_normal(Normal_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d1e8e9",
   "metadata": {},
   "source": [
    "Notice the following aspects of these results:  \n",
    "1. The empirical covariance matrix is very close in values to the covariance matrix used for the simulation.   \n",
    "2. The scatter plot shows considerable dependency between the two variables. \n",
    "3. The marginal distributions of the two variables appear to be close to Normally distributed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a3067d",
   "metadata": {},
   "source": [
    "Next, the code in the cell below does the following.  \n",
    "1. A PCA object is instantiated and the data fit.   \n",
    "2. The PCA model is used to transform or project the original data matrix into the new coordinate system.    \n",
    "3. The empirical covariance is computed and printed. \n",
    "4. **Variance ratio** of the two dimensions of the new space is computed and printed. Here, variance ratio is the variance on each dimension of the space divided by the total variance of the data. \n",
    "5. A plot of the projected data values in the new coordinate space is plotted. \n",
    "\n",
    "Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60ac2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Normal_pca = PCA().fit(Normal_random)\n",
    "simple_pca = Normal_pca.transform(Normal_random)\n",
    "print(np.cov(simple_pca[:,0], simple_pca[:,1]))\n",
    "print(Normal_pca.explained_variance_ratio_)\n",
    "plot_normal(simple_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81a149",
   "metadata": {},
   "source": [
    "The PCA transformation of these data appears to have worked as expected. Notice the following:  \n",
    "1. The diagonal terms of the covariance matrix are significantly different in value, indicating that the first component (axis) projects the majority of the variability in the data values.  \n",
    "2. The off-diagonal terms of the covariance matrix are effectively 0, indicating there is no dependency between the variables in the new coordinate system. \n",
    "3. The observation that most of the variability of the projected data are explained by the first component is confirmed by both the variance ratio values and the scatter plot. \n",
    "4. The marginal distributions of the two variables are still close to Normal, but with significantly difference scale or variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008fe1ed",
   "metadata": {},
   "source": [
    "## First Running Example  \n",
    "\n",
    "We will now start working with some simple real-world data. The famous Iris dataset was collected by a botanist named Edgar Anderson around 1935. Subsequently, the dataset became famous in data analysis circles when Ronald A Fisher used it as an example for his seminal 1936 paper on discriminate analysis, one of the first true multivariate statistical methods proposed. By modern standards this data set is small (only 150 samples) and simple (only 4 features), but the simplicity will help in understand the methods at hand.   \n",
    "\n",
    "The code in the cell below loads the data set and transforms it into a demeaned Pandas data frame with human readable column and species names. Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e98657",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = load_iris()\n",
    "\n",
    "## Demean the data columns \n",
    "temp = np.zeros(iris_data['data'].shape)\n",
    "for i in range(iris_data['data'].shape[1]): \n",
    "    temp[:,i] = np.subtract(iris_data['data'][:,i], np.mean(iris_data['data'][:,i], axis=0))\n",
    "\n",
    "## Prepare the data frame \n",
    "target_species = {0:'Setosa',1:'Versicolour',2:'Virginica'}\n",
    "species = [target_species[x] for x in iris_data['target']]\n",
    "iris = pd.DataFrame(temp, columns=['sepal_length','sepal_width','petal_length','petal_width'])\n",
    "iris['species'] = species\n",
    "iris_data = iris_data['data']\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb481bd",
   "metadata": {},
   "source": [
    "Since there are only 4 features in this dataset a pairs plot will help with understanding the relationships in these data. Execute the code below to display the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e913be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=sns.pairplot(iris, hue='species')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cbd801",
   "metadata": {},
   "source": [
    "Examine this plot array. You can see that values samples for the Setosa species are well separated. However there is some overlap between samples from Versicolour and Virginica. Further, and more importantly, it appears that these is considerable redundancy in these plots. This leads one to suspect that there is a high dependency between these cases.  \n",
    "\n",
    "We can further investigate the dependency between the variables by computing the covariance matrix. Execute the code in the cell below to compute the covariance matrix of the iris data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf4b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(np.transpose(iris_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5384f43",
   "metadata": {},
   "source": [
    "The off-diagonal terms of the covariance matrix are far from zero. We can conclude that there is significant dependency between the variables.   \n",
    "\n",
    "## Compute PCA of the iris data   \n",
    "\n",
    "The first algorithm you will apply to the iris data is linear PCA.  \n",
    "\n",
    "> **Exercise 05-1:** Compute the PCA of the iris data and plot the explained variance of the components by the following steps:  \n",
    "> 1. Instantiate a Scikit-learn PCA model object with [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).  \n",
    "> 2. Fit the model to `iris_data` numpy array using the `fit` method on the model object.  \n",
    "> 3. Create a scatter plot of the `explained_variance_ratio_` attribute of the fitted model vs. the component number.  `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f990fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b80e304",
   "metadata": {},
   "source": [
    "> Examine the plot. Does it appear that much of the variance in the data is explained by the first component? Is there any substantial difference in the variance explained between the second and third and fourth components substantially different?   \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43125d9d",
   "metadata": {},
   "source": [
    "Recall that the variance of the components from the PCA goes as the square of the singular values. You can gain another view of the relationship between the principle components by executing the code below to plot the singular values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5cf7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plt.scatter(range(1, len(iris_pca.singular_values_) + 1), iris_pca.singular_values_)\n",
    "_=plt.xlabel('Component number')\n",
    "_=plt.ylabel('Singular value')\n",
    "_=plt.title('Singular value vs. component number')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd39cce",
   "metadata": {},
   "source": [
    "Next, you will investigate the principle components used to project the data into the new space. Execute the code in the cell below to print the components.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e7948",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = iris_pca.components_\n",
    "components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce3825b",
   "metadata": {},
   "source": [
    "> **Exercise 05-2:** The principle components must be unitary (unit norm) and orthogonal. Do the following to verify these properties.  \n",
    "> 1. In the first cell below compute and print the Euclidean norm of these of the components using [numpy.linalg.norm](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html).    \n",
    "> 2. Using [itertools.combinations](https://docs.python.org/3/library/itertools.html) compute the dot (inner) product of each of pairwise combination of the components using [numpy.dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) in the second cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76476742",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4768d7c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Put your code below \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7c517",
   "metadata": {},
   "source": [
    "> Examine these results. Are these components orthogonal and unitary?   \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25a0f4e",
   "metadata": {},
   "source": [
    "> **Exercise 05-3:** From the initial exploration of the variance explained and singular values it is the case that most of the variance can be explained by the first two components. A PCA model with just 2 components will therefore explain most of the variance in the dataset. To create this model and display the results do the following:   \n",
    "> 1. Instantiate the projected data array using a PCA model object with the argument `n_components=2` and using the `fit_transform` method on the `iris_data`.   \n",
    "> 2. Plot the transformation of the data with the `plot_pca` function. Make sure you save the returned data frame as `pca_projected`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cca30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(X, labels):\n",
    "    pca_projected = pd.DataFrame(X, columns=['Component_1','Component_2'])\n",
    "    pca_projected['labels'] = labels \n",
    "    sns.scatterplot(data=pca_projected, x='Component_1', y='Component_2', hue='labels')\n",
    "    return pca_projected\n",
    "\n",
    "## Put your code below \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e5d74",
   "metadata": {},
   "source": [
    "> Examine the plot you have created. Answer the following questions:  \n",
    "> 1. How well can these clusters be linearly separated?  \n",
    "> 2. Is the range of values of the components consistent with the variance of the components?  \n",
    "> **End of Exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846d8b2",
   "metadata": {},
   "source": [
    "We can check the independence of the components by computing the covariance. Execute the code in the cell below to display the covariance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72586899",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(pca_projected.Component_1, pca_projected.Component_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaea0a8b",
   "metadata": {},
   "source": [
    "Notice that the off-diagonal components are effectively zero indicating independence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebedef94",
   "metadata": {},
   "source": [
    "## Second Example Dataset\n",
    "\n",
    "The bowl disease gene dataset has high dimensionality, with over 10,000 features. The question is can this high dimensional space be projected to a lower dimensional space.   \n",
    "\n",
    "Execute the code in the cell below to load the data set and prepare it for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba128c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_data = pd.read_csv('../data/ColonDiseaseGeneData-Cleaned.csv')\n",
    "labels = gene_data.loc[:,'Disease State']\n",
    "gene_data = gene_data.drop('Disease State', axis=1)\n",
    "\n",
    "## Demean the columns \n",
    "for col in gene_data.columns: \n",
    "    gene_data.loc[:,col] = gene_data.loc[:,col].subtract(gene_data.loc[:,col].mean(axis=0))\n",
    "\n",
    "## Display the results \n",
    "print(gene_data.shape)\n",
    "print(gene_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0175ff1e",
   "metadata": {},
   "source": [
    "For the 97 subjects there are gene expression values for over 10,000 genes.  \n",
    "\n",
    "> **Exercise 05-4:** You will now explore the ability of PCA to reduce the dimensionality of the genetics data. To test this idea do the following:   \n",
    "> 1. Instantiate a PCA object and apply the `fit` method with the `gene_data` as the argument.  \n",
    "> 2. Print the cumulative sum of the variance explained by applying the [numpy.cumsum](https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html) function to the `explained_variance_ratio_` attribute of the model object. \n",
    "> 2. Plot the `explained_variance_ratio_` attribute of the model object vs. the component number. \n",
    "> 3. Execute your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2691223",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a6be51",
   "metadata": {},
   "source": [
    "> Study your plot. Notice that the explained variance ratio decreases rapidly with the component number. Answer the following questions:  \n",
    "> 1. Does this decay indicate that dimensionality reduction can be significant?   \n",
    "> 2. Approximately how many components would you estimate are required to account for over 70% over the variance?  \n",
    "> **End of exercise.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fa0260",
   "metadata": {},
   "source": [
    "Now we will display and examine a pairwise scatter plot of the first 8 components of the PCA decomposition of the genetics data. Execute the code in the cell below to display the plot.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d02457",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gene_pca_8 = PCA(n_components=8).fit(gene_data)\n",
    "gene_pca_projected = pd.DataFrame(gene_pca_8.transform(gene_data), columns=['Component_1','Component_2','Component_3','Component_4','Component_5','Component_6','Component_7','Component_8'])\n",
    "gene_pca_projected['Disease'] = labels\n",
    "_=sns.pairplot(gene_pca_projected, hue='Disease')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a96d2a5",
   "metadata": {},
   "source": [
    "Examine the plot. Notice that most of the component values of the two disease types have significant overlap. However, in some cases there are differences in the values. This indicates that, to some extent, the disease cases are separable.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f21e52",
   "metadata": {},
   "source": [
    "## Kernel PCA\n",
    "\n",
    "Kernel PCA uses a nonlinear mapping between nonlinear sample space and a lower dimensional linear space.   \n",
    "\n",
    "> **Exercise 05-5:** You will now apply the nonlinear kernel PCA method to the iris dataset. Do the following:   \n",
    "> 1. Instantiate a kernel PCA object with the `kernel='cosine'` argument. \n",
    "> 2. Use the `fit` method with the 'iris_data' as the argument.   \n",
    "> 3. Plot the singular values, the `lambdas_` attribute of the model object, vs. the component number.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d0bb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ce10e3",
   "metadata": {},
   "source": [
    "> Examine the plot of the singular values. Does this plot indicate the dimensionality of the iris data can be reduced to 2 dimensions using the cosine kernel PCA, and why?   \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17da3987",
   "metadata": {},
   "source": [
    "> **Exercise 05-6:** Now you will create and plot the results of a kernel PCA decomposition of the iris data by the following steps:  \n",
    "> 1. Instantiate a [sklearn.decomposition.KernelPCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html) object with arguments `n_components=2` and `kernel='cosine'`.  \n",
    "> 2. Use the `fit_transform` method with the iris data as the argument to compute the projection into the new space. \n",
    "> 3. Display a plot of the two components using the `plot_pca` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77996d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5514d13e",
   "metadata": {},
   "source": [
    "> Examine the plot you have created. Answer the following questions:  \n",
    "> 1. How well can these clusters be linearly separated?  \n",
    "> 2. Does the range of values of the components indicate that most of the variance is explained by the first component? \n",
    "> **End of Exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018a7edc",
   "metadata": {},
   "source": [
    "> **Exercise 05-7:** Changing the kernel can have a significant impact on the projected components of the kernel PCA model. To see an example, repeat the steps used in the previous exercise, but the the argument `kernel='rbf'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f2074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27736e01",
   "metadata": {},
   "source": [
    "> These results are quite different. Which kernel do you think gives a more useful projection of the iris data?   \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b4157f",
   "metadata": {},
   "source": [
    "Next, we will try kernel PCA on the gene dataset. Execute the code in the cell below and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eadd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_kernel_pca = KernelPCA(n_components=8, kernel='sigmoid').fit_transform(gene_data)\n",
    "gene_pca_kernel_projected = pd.DataFrame(gene_kernel_pca, columns=['Component_1','Component_2','Component_3','Component_4','Component_5','Component_6','Component_7','Component_8'])\n",
    "gene_pca_kernel_projected['Disease'] = labels \n",
    "_=sns.pairplot(gene_pca_projected, hue='Disease')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3141812f",
   "metadata": {},
   "source": [
    "Compare this plot to the one created with linear PCA. Overall, these results seem nearly identical. It may be the case that a nonlinear transformation is not required for these data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46e261a",
   "metadata": {},
   "source": [
    "## Spectral Manifold Learning  \n",
    "\n",
    "Manifold learning seeks to map high-dimensional data onto a low-dimensional linear or nonlinear manifold. In this case we will map to a two dimensional manifold which can be displayed as a plot.  \n",
    "\n",
    "> **Exercise 05-8:** You will now apply the spectral manifold learning to the iris dataset by these steps:   \n",
    "> 1. Instantiate a [sklearn.manifold.SpectralEmbedding](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.SpectralEmbedding.html#sklearn.manifold.SpectralEmbedding) object with argument `affinity='rbf'`.\n",
    "> 2. Use the `fit_transform` method with the iris data as the argument. \n",
    "> 3. Display the result using the `plot_pca` function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d327083",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01f7154",
   "metadata": {},
   "source": [
    "> Examine this plot. Which aspects of the data are well separated and which are not?  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b3c67",
   "metadata": {},
   "source": [
    "We can also apply spectral manifold learning to the gene data. Execute the code in the cell below and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cac454",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_spectral = SpectralEmbedding(affinity='rbf').fit_transform(gene_data)\n",
    "pca_projected=plot_pca(gene_spectral, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc4d32",
   "metadata": {},
   "source": [
    "Notice that this result looks remarkably like the plot of the first two components of the linear PCA. This again indicates that the dependency relationship may be primarily linear.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dd5547",
   "metadata": {},
   "source": [
    "#### Copyright 2021, Stephen F Elston. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
